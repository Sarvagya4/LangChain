{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmKbZO4Wy/uYtkZqUZovxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarvagya4/LangChain/blob/main/Langchain7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain\n",
        "%pip install openai\n",
        "%pip install transformers"
      ],
      "metadata": {
        "id": "qUQw9H1hf0Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "gBUzvscVf4hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        ""
      ],
      "metadata": {
        "id": "AHeym4uTf6u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_id = \"EleutherAI/gpt-neo-2.7B\" #free version\n",
        "model_id = \"EleutherAI/gpt-j-6b\" #Colab PRO\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "lOVpX9t6f8LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    eos_token_id = int(tokenizer.convert_tokens_to_ids('.')),\n",
        "    device_map='auto'\n",
        ")\n",
        "assistant_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "lpTZ5S7_f-mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction how the LLM must respond the comments,\n",
        "assistant_template = \"\"\"\n",
        "You are {sentiment} social media post commenter, you will respond to the following post\n",
        "Post: \"{customer_request}\"\n",
        "Comment:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hbBhaV9EgAB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create the prompt template to use in the Chain for the first Model.\n",
        "assistant_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"sentiment\", \"customer_request\"],\n",
        "    template=assistant_template\n",
        ")"
      ],
      "metadata": {
        "id": "g1wJn3NJgDGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "assistant_chain = LLMChain(\n",
        "    llm=assistant_llm,\n",
        "    prompt=assistant_prompt_template,\n",
        "    output_key=\"assistant_response\",\n",
        "    verbose=False\n",
        ")\n",
        "#the output of the formatted prompt will pass directly to the LLM.\n",
        ""
      ],
      "metadata": {
        "id": "eNeUvXpLgEqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Support function to obtain a response to a user comment.\n",
        "def create_dialog(customer_request, sentiment):\n",
        "    #callint the .run method from the chain created Above.\n",
        "    assistant_response = assistant_chain.run(\n",
        "        {\"customer_request\": customer_request,\n",
        "        \"sentiment\": sentiment}\n",
        "    )\n",
        "    return assistant_response"
      ],
      "metadata": {
        "id": "CB2PoVlMgHFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_request = \"\"\"Your product is a piece of shit. I want my money back!\"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "vShJKkhdgIvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our assistatnt working in 'nice' mode.\n",
        "assistant_response=create_dialog(customer_request, \"nice\")\n",
        "print(f\"assistant response: {assistant_response}\")\n",
        ""
      ],
      "metadata": {
        "id": "GOfjZQ_1gLC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our assistant running in rude mode.\n",
        "assistant_response = create_dialog(customer_request, \"rude\")\n",
        "print(f\"assistant response: {assistant_response}\")\n",
        ""
      ],
      "metadata": {
        "id": "1T6d8cYrgND8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The moderator prompt template\n",
        "moderator_template = \"\"\"\n",
        "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
        "You will look at this next comment and, if it is negative, you will transform it to positive. Avoid any negative words.\n",
        "If it is nice, you will let it remain as is and repeat it word for word.\n",
        "###\n",
        "Original comment: {comment_to_moderate}\n",
        "###\n",
        "Edited comment:\"\"\"\n",
        "\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "moderator_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"comment_to_moderate\"],\n",
        "    template=moderator_template\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "JFrpAtQNgOeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_llm = assistant_llm"
      ],
      "metadata": {
        "id": "0wutv5RfgQkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We build the chain for the moderator.\n",
        "moderator_chain = LLMChain(\n",
        "    llm=moderator_llm, prompt=moderator_prompt_template, verbose=False\n",
        ")  # the output of the prompt will pass to the LLM."
      ],
      "metadata": {
        "id": "jsgmq9dDgewS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To run our chain we use the .run() command\n",
        "moderator_says = moderator_chain.run({\"comment_to_moderate\": assistant_response})\n",
        "\n",
        "print(f\"moderator_says: {moderator_says}\")"
      ],
      "metadata": {
        "id": "iYtrQpeLghXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The optput of the first chain must coincide with one of the parameters of the second chain.\n",
        "#The parameter is defined in the prompt_template.\n",
        "assistant_chain = LLMChain(\n",
        "    llm=assistant_llm,\n",
        "    prompt=assistant_prompt_template,\n",
        "    output_key=\"comment_to_moderate\",\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "#verbose True because we want to see the intermediate messages.\n",
        "moderator_chain = LLMChain(\n",
        "    llm=moderator_llm,\n",
        "    prompt=moderator_prompt_template,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "bYg72mu5gjKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Creating the SequentialChain class indicating chains and parameters.\n",
        "assistant_moderated_chain = SequentialChain(\n",
        "    chains=[assistant_chain, moderator_chain],\n",
        "    input_variables=[\"sentiment\", \"customer_request\"],\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "CM5bD16YglXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now run the chain.\n",
        "assistant_moderated_chain.run({\"sentiment\": \"rude\", \"customer_request\": customer_request})"
      ],
      "metadata": {
        "id": "JrZ5A8KIgnih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}