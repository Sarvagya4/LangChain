{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrdmcgxfxMFTrGXvzXUe6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarvagya4/LangChain/blob/main/Langchain6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ4KKLdqN9da"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain==0.1.4\n",
        "!pip install -q transformers==4.37.1\n",
        "!pip install -q accelerate==0.26.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.20.2"
      ],
      "metadata": {
        "id": "rjYI4ofYe2ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "hf_key = getpass(\"Hugging Face Key: \")\n"
      ],
      "metadata": {
        "id": "ebva6p30e46N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $hf_key"
      ],
      "metadata": {
        "id": "wr-DKHuJe6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "#from langchain.chains import LLMChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "FCvkvH2ie8xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import cuda, bfloat16"
      ],
      "metadata": {
        "id": "3nLHBwOce_Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "izu_Jp86fArm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "2sFlP7aKfDIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "NO8VmAdefEbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_key\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_key\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "MglJxwfkfG3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
        "                                          use_aut_token=hf_key)"
      ],
      "metadata": {
        "id": "HQ3AoOWRfI4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,\n",
        "    #do_sample=False,\n",
        "    top_p=0,\n",
        "    #trust_remote_code=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "assistant_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "znOastwOfLaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_template = \"\"\"\n",
        "[INST]<<SYS>>You are {sentiment} assistant that responds to user comments,\n",
        "using similar vocabulary than the user.\n",
        "Stop answering text after answer the first user.<</SYS>>\n",
        "\n",
        "User comment:{customer_request}[/INST]\n",
        "assistant_response:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dfktdJKzfN5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"sentiment\", \"customer_request\"],\n",
        "    template=assistant_template\n",
        ")"
      ],
      "metadata": {
        "id": "-KfZomQjfSuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "assistant_chain = assistant_prompt_template | assistant_llm | output_parser"
      ],
      "metadata": {
        "id": "N14be4mrfUje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dialog(customer_request, sentiment):\n",
        "    #calling the .invoke method from the chain created Above.\n",
        "    assistant_response = assistant_chain.invoke(\n",
        "        {\"customer_request\": customer_request,\n",
        "        \"sentiment\": sentiment}\n",
        "    )\n",
        "    return assistant_response"
      ],
      "metadata": {
        "id": "Ry4dl__AfXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_request = \"\"\"Your product is a piece of shit. I want my money back!\"\"\""
      ],
      "metadata": {
        "id": "U5ENpNsjfZny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_response=create_dialog(customer_request, \"nice\")\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "It9ztEKXfbds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_response = create_dialog(customer_request, \"most rude possible assistant\")\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "bNy1LDJ-fcxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_template = \"\"\"\n",
        "[INST]<<SYS>>You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
        "You will receive an original comment and if it is impolite you must transform into polite.\n",
        "Try to mantain the meaning when possible.<</SYS>>\n",
        "\n",
        "Original comment: {comment_to_moderate}/[INST]\n",
        "\"\"\"\n",
        "\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "moderator_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"comment_to_moderate\"],\n",
        "    template=moderator_template\n",
        ")"
      ],
      "metadata": {
        "id": "IfdAES23fegr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_llm = assistant_llm"
      ],
      "metadata": {
        "id": "ERws7E6pfhFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_chain = moderator_prompt_template | moderator_llm | output_parser"
      ],
      "metadata": {
        "id": "YfTkVQeufiuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_says = moderator_chain.invoke({\"comment_to_moderate\": assistant_response})\n",
        "\n",
        "print(moderator_says)"
      ],
      "metadata": {
        "id": "_iPxD9lSfkO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_moderated_chain = (\n",
        "    {\"comment_to_moderate\":assistant_chain}\n",
        "    |moderator_chain\n",
        ")"
      ],
      "metadata": {
        "id": "8hw6z45Qfl1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "assistant_moderated_chain.invoke({\"sentiment\": \"really rude\", \"customer_request\": customer_request},\n",
        "                                 config={'callbacks':[ConsoleCallbackHandler()]})"
      ],
      "metadata": {
        "id": "Y-Aot5gOfoZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}